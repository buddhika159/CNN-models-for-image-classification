{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57896088",
   "metadata": {},
   "source": [
    "### Building Fine-tuned VGG16 model\n",
    "\n",
    "To overcome this overfitting problem, we can fine tune a pre-trained model to classify images. Most famouse pre-trained model for image classificaiton is called `VGG16` which came out as a winner of 2014 ImageNet competition classifying 1000 image classes. `cat` and `dog` were amoung them.\n",
    "\n",
    "In data preprocessing phase we used a `preprocessing_function` that `VGG16` used. This is nothing but substracting the mean RGB of training set from each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1e2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69adfed7",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66e41971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n",
      "Found 100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = 'data/dogs-vs-cats/train'\n",
    "valid_path = 'data/dogs-vs-cats/valid'\n",
    "test_path = 'data/dogs-vs-cats/test'\n",
    "train_batches = ImageDataGenerator(preprocessing_function = tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory = train_path, target_size = (224,224), classes = ['cat', 'dog'], batch_size = 10)\n",
    "valid_batches = ImageDataGenerator(preprocessing_function = tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory = valid_path, target_size = (224,224), classes = ['cat', 'dog'], batch_size = 10)\n",
    "test_batches = ImageDataGenerator(preprocessing_function = tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory = test_path, target_size = (224,224), classes = ['cat', 'dog'], batch_size = 10, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e7a72",
   "metadata": {},
   "source": [
    "### Download the VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6599efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = tf.keras.applications.vgg16.VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66691c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4d0cb",
   "metadata": {},
   "source": [
    "### Fine-tune the VGG16 model\n",
    "As this `VGG16` model is heavily dense, our objective of the fine-tune is to remove the last layer and train the model in to over cat-vs-dog dataset to have only 2 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b746792f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.functional.Functional"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vgg16_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df337a04",
   "metadata": {},
   "source": [
    "`vgg16_model` is functional type model. To convert it to a sequential model, we iterate through all layers except the last one and add the weights to a new sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4788372",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "for layer in vgg16_model.layers[:-1]:\n",
    "    model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7130dcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df30de",
   "metadata": {},
   "source": [
    "Now with these parameters we can train out dataset to get only two outputs. When we go through the traing process with this model, we need to freeze all the trainable parameters by seting `layer.trainable` to `False`.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd113999",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2cfd05",
   "metadata": {},
   "source": [
    "Lets add the output layer which will be the only trainable layer in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1392f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a3918ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 8,194\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679db9b",
   "metadata": {},
   "source": [
    "### Train the fine-tuned VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e141fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(learning_rate = 0.0001),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecb61ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 - 425s - loss: 0.3105 - accuracy: 0.8670 - val_loss: 0.1086 - val_accuracy: 0.9550 - 425s/epoch - 4s/step\n",
      "Epoch 2/5\n",
      "100/100 - 842s - loss: 0.0843 - accuracy: 0.9750 - val_loss: 0.0906 - val_accuracy: 0.9550 - 842s/epoch - 8s/step\n",
      "Epoch 3/5\n",
      "100/100 - 442s - loss: 0.0617 - accuracy: 0.9800 - val_loss: 0.0767 - val_accuracy: 0.9700 - 442s/epoch - 4s/step\n",
      "Epoch 4/5\n",
      "100/100 - 449s - loss: 0.0467 - accuracy: 0.9880 - val_loss: 0.0818 - val_accuracy: 0.9650 - 449s/epoch - 4s/step\n",
      "Epoch 5/5\n",
      "100/100 - 445s - loss: 0.0375 - accuracy: 0.9930 - val_loss: 0.0772 - val_accuracy: 0.9650 - 445s/epoch - 4s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e3fc7aaf40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_batches, \n",
    "          validation_data = valid_batches, \n",
    "          epochs = 5, \n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5350d",
   "metadata": {},
   "source": [
    "### Predicting with fine-tuned VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceaaaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x = test_batches, verbose = 0)\n",
    "predictions = np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0736daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true = test_batches.classes, \n",
    "                      y_pred = np.argmax(predictions, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5775158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm,\n",
    "                              display_labels = test_batches.class_indices)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce2395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
